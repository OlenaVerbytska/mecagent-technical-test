{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21bac548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import ViTImageProcessor, VisionEncoderDecoderModel\n",
    "import re\n",
    "from collections import Counter\n",
    "import torch\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel, \n",
    "    ViTConfig, \n",
    "    GPT2Config,\n",
    "    ViTImageProcessor,\n",
    "    GPT2Tokenizer,\n",
    "    ViTModel,\n",
    "    GPT2LMHeadModel\n",
    ")\n",
    "from typing import List, Dict, Any\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f242489",
   "metadata": {},
   "source": [
    "# 1. Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c898ad8",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8c92409",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"CADCODER/GenCAD-Code\", num_proc=16, split=[\"train\", \"test\"], cache_dir=\"/Volumes/BIG-DATA/HUGGINGFACE_CACHE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94be457",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2362028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': Image(mode=None, decode=True, id=None), 'deepcad_id': Value(dtype='string', id=None), 'cadquery': Value(dtype='string', id=None), 'token_count': Value(dtype='int64', id=None), 'prompt': Value(dtype='string', id=None), 'hundred_subset': Value(dtype='bool', id=None)}\n",
      "147289 7355\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000/00001378\n",
      "_________________________\n",
      "import cadquery as cq\n",
      "# Generating a workplane for sketch 0\n",
      "wp_sketch0 = cq.Workplane(cq.Plane(cq.Vector(0.0, -0.1953125, 0.0), cq.Vector(1.0, 0.0, 0.0), cq.Vector(0.0, 0.0, 1.0)))\n",
      "loop0=wp_sketch0.moveTo(0.17730263157894738, 0.0).lineTo(0.17730263157894738, 0.38281250000000006).lineTo(0.0, 0.38281250000000006).lineTo(0.0, 0.0).close()\n",
      "solid0=wp_sketch0.add(loop0).extrude(0.3046875)\n",
      "solid=solid0\n",
      "# Generating a workplane for sketch 1\n",
      "wp_sketch1 = cq.Workplane(cq.Plane(cq.Vector(0.1796875, -0.109375, 0.0), cq.Vector(1.0, 0.0, 0.0), cq.Vector(0.0, 0.0, 1.0)))\n",
      "loop1=wp_sketch1.moveTo(0.3782072368421052, 0.0).lineTo(0.5703125, 0.1140625).lineTo(0.3782072368421052, 0.22212171052631577).lineTo(0.0, 0.22212171052631577).lineTo(0.0, 0.0).close()\n",
      "solid1=wp_sketch1.add(loop1).extrude(0.1796875)\n",
      "solid=solid.union(solid1)\n",
      "\n",
      "_________________________\n",
      "The number of characters: 821\n",
      "The number of tokens: 1195\n",
      "_________________________\n",
      "Generate the CADQuery code needed to create the CAD for the provided image. Just the code, no other words.\n",
      "_________________________\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "train = ds[0]\n",
    "test = ds[1]\n",
    "print(train.features)\n",
    "print(len(train), len(test))\n",
    "\n",
    "example = train[1]\n",
    "img = example['image']\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(example['deepcad_id'])\n",
    "print(\"_________________________\")\n",
    "print(example['cadquery'])\n",
    "print(\"_________________________\")\n",
    "print(\"The number of characters:\", len(example['cadquery']))\n",
    "print(\"The number of tokens:\", example['token_count'])\n",
    "print(\"_________________________\")\n",
    "print(example['prompt'])\n",
    "print(\"_________________________\")\n",
    "print(example['hundred_subset'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8047335c",
   "metadata": {},
   "source": [
    "The number of tokens is greater than number of characters in the query. More information about the tokenization is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad2ebfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique prompts: 1\n",
      "All prompts are the same.\n"
     ]
    }
   ],
   "source": [
    "all_prompts = train['prompt'] + test['prompt']\n",
    "unique_prompts = set(all_prompts)\n",
    "print(f\"Number of unique prompts: {len(unique_prompts)}\")\n",
    "if len(unique_prompts) == 1:\n",
    "    print(\"All prompts are the same.\")\n",
    "else:\n",
    "    print(\"There are different prompts in the dataset.\")\n",
    "    print(\"Sample unique prompts:\", list(unique_prompts)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30615ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique image sizes: {(448, 448)}\n",
      "Total images: 1000\n"
     ]
    }
   ],
   "source": [
    "image_sizes = [img.size for img in train[:1000]['image']]\n",
    "unique_sizes = set(image_sizes)\n",
    "print(f\"Unique image sizes: {unique_sizes}\")\n",
    "print(f\"Total images: {len(image_sizes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71a894f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count statistics:\n",
      "  Mean: 1257.47\n",
      "  Median: 1064.00\n",
      "  Min: 807\n",
      "  Max: 6495\n"
     ]
    }
   ],
   "source": [
    "# Token count statistics\n",
    "token_counts = train['token_count'] + test['token_count']\n",
    "print(f\"Token count statistics:\")\n",
    "print(f\"  Mean: {np.mean(token_counts):.2f}\")\n",
    "print(f\"  Median: {np.median(token_counts):.2f}\")\n",
    "print(f\"  Min: {np.min(token_counts)}\")\n",
    "print(f\"  Max: {np.max(token_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaa969bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CAD code length statistics:\n",
      "  Mean: 882.21 characters\n",
      "  Median: 605.00 characters\n",
      "  Min: 283 characters\n",
      "  Max: 8104 characters\n",
      "\n",
      "Hundred subset: 100 out of 154644 samples\n"
     ]
    }
   ],
   "source": [
    "# CAD code length analysis\n",
    "cad_lengths = [len(code) for code in train['cadquery'] + test['cadquery']]\n",
    "print(f\"\\nCAD code length statistics:\")\n",
    "print(f\"  Mean: {np.mean(cad_lengths):.2f} characters\")\n",
    "print(f\"  Median: {np.median(cad_lengths):.2f} characters\")\n",
    "print(f\"  Min: {np.min(cad_lengths)} characters\")\n",
    "print(f\"  Max: {np.max(cad_lengths)} characters\")\n",
    "\n",
    "# Check subset distribution\n",
    "subset_counts = sum(train['hundred_subset'] + test['hundred_subset'])\n",
    "print(f\"\\nHundred subset: {subset_counts} out of {len(train) + len(test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed502736",
   "metadata": {},
   "source": [
    "# 2. Baseline \n",
    "As I'm new to working directly with transformer models, I began by experimenting with a basic encoder-decoder architecture to validate the pipeline. Here, I used ViTModel as the image encoder and GPT-2 as the text decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b89f3b0",
   "metadata": {},
   "source": [
    "## CAD code generator\n",
    "\n",
    "We'll add CAD-specific tokens, because \n",
    "- CAD code has specific repetitive syntax \n",
    "- our dataset is large enough to benefit from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f0dfec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CAD Code Generator...\n",
      "Generated 63 CAD-specific tokens\n",
      "['wp_sketch1 =', '.wire(', 'loop1=', 'wp_sketch0', '.close(', 'loop3', 'solid2', '# Generating a workplane for sketch 7', 'loop0', 'wp_sketch', '# Generating a workplane for sketch 3', '.cut(', 'import cadquery as cq', '# Generating a workplane for sketch 6', 'wp_sketch3', 'loop2', '.Plane(', '.extrude(', '.intersect(', '# workplane', '.rectangle(', '# Creating', '.union(', '# Generating a workplane for sketch 2', '.add(', 'solid', '# Generating a workplane for sketch 0', 'loop2=', 'solid1=', 'wp_sketch1', '# Generating a workplane for sketch 4', '.revolve(', 'solid=', '.solid(', '.Workplane(', 'cadquery', 'cq.Vector', '# sketch', '.circle(', 'loop0=', '.fillet(', '.moveTo(', '.face(', 'cq.Workplane', '.lineTo(', '.Vector(', 'cq.Plane', '.subtract(', 'wp_sketch2', '# Generating a workplane for sketch 1', 'cq', '# Generating', 'loop1', 'loop', '# Generating a workplane for sketch 5', '.chamfer(', 'result', '.threePointArc(', '# Generating a workplane for sketch 8', 'wp_sketch0 =', 'solid0=', 'solid0', 'solid1']\n"
     ]
    }
   ],
   "source": [
    "class CADCodeGenerator:\n",
    "    def __init__(self, cad_codes, vocab_size=None, max_length=1024):\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token # needed for compatibility\n",
    "        \n",
    "        # CAD-specific tokens\n",
    "        def generate_cad_specific_tokens(cad_codes, min_frequency=10):\n",
    "            import_patterns = []\n",
    "            method_patterns = []\n",
    "            operation_patterns = []\n",
    "            variable_patterns = []\n",
    "            comment_patterns = []\n",
    "            \n",
    "            for code in cad_codes:\n",
    "                # Extract import statements\n",
    "                imports = re.findall(r'import\\s+\\w+(?:\\s+as\\s+\\w+)?', code)\n",
    "                import_patterns.extend(imports)\n",
    "                \n",
    "                # Extract CAD method calls (cq.something)\n",
    "                methods = re.findall(r'cq\\.\\w+', code)\n",
    "                method_patterns.extend(methods)\n",
    "                \n",
    "                # Extract function calls (.functionName())\n",
    "                function_calls = re.findall(r'\\.\\w+\\(', code)\n",
    "                operation_patterns.extend(function_calls)\n",
    "\n",
    "                # Extract variable names (wp_sketch, solid, loop)\n",
    "                variables = re.findall(r'\\b(?:wp_sketch|solid|loop)\\d*\\b', code)\n",
    "                variable_patterns.extend(variables)\n",
    "                \n",
    "                # Extract assignment patterns (variable = )\n",
    "                assignments = re.findall(r'\\b\\w+\\s*=', code)\n",
    "                variable_patterns.extend([a.strip() for a in assignments])\n",
    "                \n",
    "                # Extract comment keywords\n",
    "                comments = re.findall(r'#\\s*(\\w+(?:\\s+\\w+)*)\\s*$', code, re.MULTILINE)\n",
    "                comment_patterns.extend([f\"# {comment}\" for comment in comments])\n",
    "            \n",
    "            # Count frequencies\n",
    "            all_patterns = {\n",
    "                'imports': Counter(import_patterns),\n",
    "                'methods': Counter(method_patterns),\n",
    "                'operations': Counter(operation_patterns),\n",
    "                'variables': Counter(variable_patterns),\n",
    "                'comments': Counter(comment_patterns)\n",
    "            }\n",
    "            \n",
    "            # Collect high-frequency tokens\n",
    "            cad_tokens = []\n",
    "            \n",
    "            for _, counter in all_patterns.items():\n",
    "                frequent_tokens = [token for token, freq in counter.most_common(20) if freq >= min_frequency]\n",
    "                cad_tokens.extend(frequent_tokens)\n",
    "            \n",
    "            # Essential CAD tokens, even if not frequent\n",
    "            essential_tokens = [\n",
    "                # Core CAD imports\n",
    "                'import cadquery as cq',\n",
    "                'cadquery',\n",
    "                'cq',\n",
    "                \n",
    "                # Basic CAD objects\n",
    "                'cq.Workplane',\n",
    "                'cq.Plane',\n",
    "                'cq.Vector',\n",
    "                \n",
    "                # Common operations\n",
    "                '.moveTo(',\n",
    "                '.lineTo(',\n",
    "                '.circle(',\n",
    "                '.rectangle(',\n",
    "                '.extrude(',\n",
    "                '.revolve(',\n",
    "                '.union(',\n",
    "                '.cut(',\n",
    "                '.intersect(',\n",
    "                '.fillet(',\n",
    "                '.chamfer(',\n",
    "                \n",
    "                # 3D operations\n",
    "                '.add(',\n",
    "                '.subtract(',\n",
    "                '.threePointArc(',\n",
    "                '.close(',\n",
    "                '.wire(',\n",
    "                '.face(',\n",
    "                '.solid(',\n",
    "                \n",
    "                # Variable patterns\n",
    "                'wp_sketch',\n",
    "                'solid',\n",
    "                'loop',\n",
    "                'result',\n",
    "                \n",
    "                # Comments\n",
    "                '# Generating',\n",
    "                '# Creating',\n",
    "                '# sketch',\n",
    "                '# workplane',\n",
    "            ]\n",
    "\n",
    "            all_tokens = list(set(cad_tokens + essential_tokens))\n",
    "            return all_tokens\n",
    "\n",
    "\n",
    "        special_tokens = generate_cad_specific_tokens(cad_codes[:1000], min_frequency=5)\n",
    "        print(f\"Generated {len(special_tokens)} CAD-specific tokens\")\n",
    "        print(special_tokens)\n",
    "        self.tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
    "        \n",
    "        # Update vocab size\n",
    "        vocab_size = len(self.tokenizer)\n",
    "        \n",
    "        # Configure Vision Encoder (ViT)\n",
    "        encoder_config = ViTConfig(\n",
    "            image_size=448,  # Based on the dataset analysis\n",
    "            patch_size=16,\n",
    "            num_channels=3,\n",
    "            hidden_size=768,\n",
    "            num_hidden_layers=12,\n",
    "            num_attention_heads=12,\n",
    "            intermediate_size=3072,\n",
    "        )\n",
    "        \n",
    "        # Configure Text Decoder (GPT2-based)\n",
    "        decoder_config = GPT2Config(\n",
    "            vocab_size=vocab_size,\n",
    "            n_positions=max_length,\n",
    "            n_embd=768,\n",
    "            n_layer=12,\n",
    "            n_head=12,\n",
    "            max_length=max_length,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            bos_token_id=self.tokenizer.bos_token_id,\n",
    "add_cross_attention=True,  # Essential for vision-encoder-decoder\n",
    "            is_decoder=True,           # Enable decoder mode\n",
    "        )\n",
    "        \n",
    "        # Create the encoder and decoder models\n",
    "        encoder = ViTModel(encoder_config)\n",
    "        decoder = GPT2LMHeadModel(decoder_config)\n",
    "        \n",
    "        # Create Vision-Encoder-Decoder Model\n",
    "        self.model = VisionEncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
    "        \n",
    "        # Configure generation parameters\n",
    "        self.model.config.decoder_start_token_id = self.tokenizer.bos_token_id\n",
    "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "        self.model.config.eos_token_id = self.tokenizer.eos_token_id\n",
    "        self.model.config.max_length = max_length\n",
    "        self.model.config.early_stopping = True\n",
    "        self.model.config.no_repeat_ngram_size = 3\n",
    "        self.model.config.length_penalty = 2.0\n",
    "        self.model.config.num_beams = 4\n",
    "        \n",
    "        # Image processor\n",
    "        self.image_processor = ViTImageProcessor(\n",
    "            size={\"height\": 448, \"width\": 448},\n",
    "            do_resize=True,\n",
    "            do_normalize=True,\n",
    "            image_mean=[0.485, 0.456, 0.406],\n",
    "            image_std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "        \n",
    "    def preprocess_data(self, examples):\n",
    "        \"\"\"Preprocess images and CAD code for training\"\"\"\n",
    "        # Process images\n",
    "        images = [img.convert('RGB') for img in examples['image']]\n",
    "        pixel_values = self.image_processor(images, return_tensors=\"pt\")['pixel_values'] \n",
    "        \n",
    "        # Tokenize CAD code\n",
    "        cad_codes = examples['cadquery']\n",
    "        tokenized = self.tokenizer(\n",
    "            cad_codes,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Prepare labels \n",
    "        labels = tokenized['input_ids'].clone()\n",
    "        # Replace padding tokens with -100 (ignore in loss)\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': labels,\n",
    "            'decoder_attention_mask': tokenized['attention_mask']\n",
    "        }\n",
    "    \n",
    "    def generate_cad_code(self, image, max_length=None):\n",
    "        \"\"\"Generate CAD code from an image\"\"\"\n",
    "        if max_length is None:\n",
    "            max_length = self.max_length\n",
    "            \n",
    "        # Preprocess image\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image).convert('RGB')\n",
    "        \n",
    "        pixel_values = self.image_processor(image, return_tensors=\"pt\")['pixel_values']\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            generated_ids = self.model.generate(\n",
    "                pixel_values,\n",
    "                max_length=max_length,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=3,\n",
    "                length_penalty=2.0\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "\n",
    "# Initialize the model\n",
    "print(\"Initializing CAD Code Generator...\")\n",
    "cad_generator = CADCodeGenerator(train['cadquery'][:10000])  # Use a sample of CAD codes for initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cc43c6",
   "metadata": {},
   "source": [
    "## Custom data collator\n",
    "\n",
    "We have to create a custom data collator, since HuggingFace's default collators are designed for homogeneous data types. In our case, we have tensors and strings which have to be preprocessed differently. CAD code requires proper padding, while images should be normalized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5482a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CADDataCollator:    \n",
    "    def __init__(self, tokenizer, image_processor, max_length=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        # Extract images and CAD codes\n",
    "        images = [item['image'] for item in batch]\n",
    "        cad_codes = [item['cadquery'] for item in batch]\n",
    "        \n",
    "        # Process images (convert to RGB if needed)\n",
    "        rgb_images = [img.convert('RGB') if img.mode != 'RGB' else img for img in images]\n",
    "        \n",
    "        # Batch process images\n",
    "        image_inputs = self.image_processor(\n",
    "            rgb_images, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        pixel_values = image_inputs['pixel_values']\n",
    "        \n",
    "        # Tokenize CAD codes with padding\n",
    "        text_inputs = self.tokenizer(\n",
    "            cad_codes,\n",
    "            padding=True,  # Pad to longest in batch\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Prepare labels (shift right for causal LM)\n",
    "        labels = text_inputs['input_ids'].clone()\n",
    "        # Replace padding tokens with -100 (ignore in loss)\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': labels,\n",
    "            'decoder_attention_mask': text_inputs['attention_mask']\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f6e170",
   "metadata": {},
   "source": [
    "## Training on a subset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9764bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = \"./cad-model-checkpoints\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # Learning & Optimization\n",
    "    learning_rate=5e-5,              # Conservative for vision-language\n",
    "    warmup_steps=500,                # Gradual warmup\n",
    "    weight_decay=0.01,               # Regularization\n",
    "    \n",
    "    # Batch & Memory Management  \n",
    "    per_device_train_batch_size=2,   # Small due to memory constraints\n",
    "    per_device_eval_batch_size=2,    \n",
    "    gradient_accumulation_steps=8,   # Effective batch size = 2*8 = 16\n",
    "    dataloader_num_workers=0,        # Start with 0, increase if stable\n",
    "    \n",
    "    # Training Schedule\n",
    "    num_train_epochs=3,              # Start with few epochs\n",
    "    max_steps=-1,                    # Use num_epochs instead\n",
    "    \n",
    "    # Evaluation & Saving\n",
    "    eval_strategy=\"steps\",           # Evaluate during training\n",
    "    eval_steps=500,                  # Every 500 steps\n",
    "    save_strategy=\"steps\",           # Save checkpoints regularly\n",
    "    save_steps=500,                  # Every 500 steps\n",
    "    save_total_limit=3,              # Keep only last 3 checkpoints\n",
    "    \n",
    "    # Monitoring & Debugging\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_steps=100,               # Log every 100 steps\n",
    "    report_to=None,                  # Disable wandb for now\n",
    "    \n",
    "    # Performance & Stability\n",
    "    fp16=True,                       # Mixed precision (if supported)\n",
    "    gradient_checkpointing=True,     # Trade compute for memory\n",
    "    \n",
    "    # Early Stopping & Best Model\n",
    "    load_best_model_at_end=True,     # Load best checkpoint at end\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,         # Lower loss is better\n",
    "\n",
    "    # Data Processing\n",
    "    remove_unused_columns=False,     # Keep all columns for custom data collator\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    ")\n",
    "\n",
    "# Prepare subsets of datasets for training\n",
    "def create_dataset_splits(train_data, test_data, max_train_samples=1000, max_eval_samples=100):\n",
    "    \"\"\"Create smaller dataset splits for initial training/testing\"\"\"\n",
    "    \n",
    "    # Use subset for faster iteration during development\n",
    "    train_subset = train_data.select(range(min(max_train_samples, len(train_data))))\n",
    "    eval_subset = test_data.select(range(min(max_eval_samples, len(test_data))))\n",
    "    \n",
    "    return train_subset, eval_subset\n",
    "\n",
    "# Create dataset splits\n",
    "train_dataset, eval_dataset = create_dataset_splits(train, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "373eb217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eleon\\AppData\\Local\\Temp\\ipykernel_16080\\3916487158.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\eleon\\anaconda3\\envs\\echo-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu :')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/189 : < :, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m :\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eleon\\anaconda3\\envs\\echo-env\\Lib\\site-packages\\transformers\\trainer.py:2207\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2205\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2208\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2212\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eleon\\anaconda3\\envs\\echo-env\\Lib\\site-packages\\transformers\\trainer.py:2549\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2542\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2543\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2544\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2547\u001b[0m )\n\u001b[0;32m   2548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2549\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2552\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2554\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2555\u001b[0m ):\n\u001b[0;32m   2556\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2557\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\eleon\\anaconda3\\envs\\echo-env\\Lib\\site-packages\\transformers\\trainer.py:3798\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   3796\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 3798\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3800\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\eleon\\anaconda3\\envs\\echo-env\\Lib\\site-packages\\accelerate\\accelerator.py:2553\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2553\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eleon\\anaconda3\\envs\\echo-env\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eleon\\anaconda3\\envs\\echo-env\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eleon\\anaconda3\\envs\\echo-env\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32mc:\\Users\\eleon\\anaconda3\\envs\\echo-env\\Lib\\site-packages\\torch\\autograd\\function.py:307\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m     )\n\u001b[0;32m    306\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eleon\\anaconda3\\envs\\echo-env\\Lib\\site-packages\\torch\\utils\\checkpoint.py:320\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[1;34m(ctx, *args)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs_with_grad) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone of output has requires_grad=True,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m this checkpoint() is not necessary\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    319\u001b[0m     )\n\u001b[1;32m--> 320\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_with_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_with_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m    322\u001b[0m     inp\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inp, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m detached_inputs\n\u001b[0;32m    324\u001b[0m )\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m+\u001b[39m grads\n",
      "File \u001b[1;32mc:\\Users\\eleon\\anaconda3\\envs\\echo-env\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eleon\\anaconda3\\envs\\echo-env\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the data collator\n",
    "data_collator = CADDataCollator(\n",
    "    tokenizer=cad_generator.tokenizer,\n",
    "    image_processor=cad_generator.image_processor,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with corrected model\n",
    "trainer = Trainer(\n",
    "    model=cad_generator.model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=cad_generator.tokenizer,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device {device} :')\")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35020ad",
   "metadata": {},
   "source": [
    "## After training\n",
    "I would evaluate Valid Syntax Rate and Best IOU, as suggested.\n",
    "Also I wonder if we could apply some standard computer vision techniques for evaluation. For example, consider the substraction result of the ground truth and the image that is generated from the predicted code. It might work, if the original images are generated from some code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd04f19",
   "metadata": {},
   "source": [
    "# How can I improve the baseline?\n",
    "\n",
    "1. I can use a CNN pretrained on a large dataset and fine-tune it to obtain a better image encoder. Additionally, its possible to combine features from different layers of the ViT model.\n",
    "2. For the decoder, I can experiment with larger language models and study CAD code patterns more thoroughly.\n",
    "3. I can explore more effective cross-attention mechanisms between the vision and text modalities.\n",
    "4. I could also design a custom multi-objective loss function, similar to those used in YOLO models.\n",
    "5. There's more, but I ran out of time :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee1d99d",
   "metadata": {},
   "source": [
    "# To read later\n",
    "\n",
    "- https://arxiv.org/pdf/2105.09492\n",
    "- https://arxiv.org/pdf/2503.23062"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "echo-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
